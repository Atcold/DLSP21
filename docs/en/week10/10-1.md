---
lang-ref: ch.10-1
title: Self Supervised Learning in Computer Vision
lecturer: Ishan Mishra
authors: Srishti Bhargava, Jude Naveen Raj Ilango
date: 16 May 2021
---

## Self Supervised Learning for Computer Vision 

In the recent years, one of the major success factors for many different Computer Vision problems has been learning visual representations by performing supervised learning. And, using these learned representations, or learned model weights as initializations for other computer vision tasks, where a large quantum of labelled data might not be available.

However, getting real labels is a difficult and expensive process. For instance, the popular Imagenet dataset consisting of about 14 million images and 22,000 categories roughly took about 22 human years to label. 

Because of this, the community started to look for alternate labelling processes, such as hashtags for social media images, GPS locations, or self-supervised approaches where the label is a property of the data sample itself.

### Pretext Task 

The pretext task is a self-supervised learning task solved to learn visual representations, with the aim of using the learned representations or model weights obtained in the process, for other downstream tasks.

A pretext task can be developed using images, video, or video and sound. In each pretext task, there is a part of visible and hidden data, and the task is to predict either the hidden data or some property of the hidden data.

Popular pretext tasks for images:
1. Predicting relative position of patches
    * Consider two patches - blue and red and sample them randomly on the image. The task is to predict relative position of one patch with respect to the other patch. 
    * The image patches are fed into the Siamese Network and concatenated to solve the classification problem. 
    * The intention here is to use this task as a way to learn features. 

2. Jigsaw Puzzles 
    * Permute nine image patches. 
    * Task is to basically classify which permutation was applied. 

3. Predicting Rotations
    * Apply one of the four rotations to an image.
    * Task is to predict the rotation type. 

**What is missing from pretext tasks?**
 
There is a fairly big mismatch between what is being solved in the pretext tasks and what needs to be achieved by the transfer tasks (classifying examples/detecting objects). 
Thus, this pre-training is not suitable for final tasks. 

One possible way to fix is to apply linear classifiers to the intermediate layers to figure out the features at each of the layers, once the pre-trained network is obtained. This way, performance at each layer can be assessed to figure out which layer's features to use for the transfer tasks. 

Pre-trained features should satisfy two fundamental properties:
1. Useful to represent how images relate to one another (Picture of 2 trees, should be able to figure out that they are the same concept)
2. Be robust to nuisance factors (should be able to differentiate trees in different conditions - weather, across years, lighting, etc)

### Popular method for self-supervised learning
In the recent years, a popular and common method for self supervised learning is to learn features that are robust to data augmentation. 
Learn features such that, 

<div>
$$
\begin{aligned}
    f_{\theta}(I) = f_{\theta}(augment(I))
\end{aligned}
$$
</div>

Features produced by the network for an image should be stable under different types of data augmentation techniques. 
This property is extremely useful because the features are going to be invariant to the nuisance factors.

### Trivial solution 
<img src="{{site.baseurl}}/images/week10/10-1/ssl_trivial.png" style="width: 350px; background-color:#DCDCDC;" /><br>

Take an image, apply two different data augmentations to it, feed it through encoder, compute the similarity. Get the gradients and backpropagate. 

This network will essentially ignore the image input and produce a constant representation of the image. This will satisfy the invariance property but it is not useful because it can produce the same features no matter what image is being fed into it. The representations collapse and become unusable for downstream recognition tasks because it is not going to capture how images are related to one another. 

Most of the recent self-supervised methods can be categorized by the way that they are avoiding trivial solutions:

### Categorization of recent self-supervised methods:

1. Maximize Similarity Objective (Contrastive Learning, Clustering, Distillation)
2. Redundancy Reduction Objective

In order to evaluate these networks, a subset of the Imagenet dataset (1.3 million images over 1000 categories) without labels is used. And pre-training is performed using a randomly initialized Resnet-50 model.

**What are the different ways to perform transfer learning and evaluate its performance?**

We can do one of 2 things:
1. Train a linear classifier on top of the fixed features output by the pretrained network. This may be suitable for a classification task.
2. Finetune the entire pretrained network on the target task. This may be more suitable for a detection task.

## Contrastive Learning 

**Loss function:** Embeddings from related images should be closer than embeddings from unrelated images.

<img src="{{site.baseurl}}/images/week10/10-1/con_learning.png" style="width: 400px; background-color:#DCDCDC;" /><br>


Different Contrastive Learning methods include:

1. Pretext-Invariant Representation Learning (PIRL) 
2. SimCLR
3. MoCo




## Pretext-Invariant Representation Learning (PIRL) 

<img src="{{site.baseurl}}/images/week10/10-1/pirl.png" style="width: 400px; background-color:#DCDCDC;" /><br>

We feed an image $I$ and augmented image $I^t$ through ConvNet and use the contrastive learning loss function on top to encourage similarity. This image transform is going to serve as a pretext task. By doing so we aim for the network to learn such that it would be invariant to the pretext task. 


<img src="{{site.baseurl}}/images/week10/10-1/cl_loss_fn.png" style="width: 400px; background-color:#DCDCDC;" /><br>

The aim is to achieve high similarity for image and patch features belonging to the same image. And low similarity for features from other random images. The pretext task of PIRL tries to achieve invariance over data augmentations rather than predicting the data augmentation.

### Semantic Features

<div style="margin-left: 40px;"><img src="{{site.baseurl}}/images/week10/10-1/semantic_features.png" style="width: 350px; background-color:#DCDCDC;" /><br></div>
- The graph compares the performance of two networks, using a linear classifier at each layer to probe and compute the accuracy. One is trained using PIRL and other trained using Jigsaw.
-  Difference is in the way they are treating the pretext task. Jigsaw is trying to predict the permutation and PIRL is trying to be invariant to it. 
- Performance for PIRL keeps increasing indicating that the feature is becoming increasingly aligned to the downstream classification tasks. Whereas in the Jigsaw network, the performance plateaues and then drops down sharply. Since Jigsaw is trying to retain pretext information, it ends up not performing well for the transfer tasks. 

## Sampling Positives and Negatives
There are many ways to obtain samples that make up related (positive) and unrelated (negative) pairs:
1. **Crops of an image:** Contrastive Predictive Coding (CPC) based models use patches from one image that are close-by as positives (related) and patches that are far-away from each other as negatives (unrelated).

    <img src="{{site.baseurl}}/images/week10/10-1/CL_same_image.png" style="width: 300px; text-align: center; background-color:#DCDCDC;" /><br>
2. **Crops across images:** Patches from within one image are considered related and patches from 2 different images are considered unrelated. Used in MoCo and SimCLR

    <img src="{{site.baseurl}}/images/week10/10-1/CL_multi_image.png" style="width: 300px; text-align: center; background-color:#DCDCDC;" /><br>
3. **Videos:** Frames that are closeby in temporal space are related, and those far-away are unrelated
   
   <img src="{{site.baseurl}}/images/week10/10-1/CL_video.png" style="width: 300px; text-align: center; background-color:#DCDCDC;" /><br>
4. **Multimodal - Video and Audio:** Video sequences along with their corresponding audio are related. Video sequences from one video and audio from another are unrelated.
   
    <img src="{{site.baseurl}}/images/week10/10-1/CL_video_audio.png" style="width: 300px; text-align: center; background-color:#DCDCDC;" /><br>

5. **Video Object Tracking:** An object is tracked through multiple frames in a video. Detection patches from the same video are related. Detection patches from different videos are unrelated.

   <img src="{{site.baseurl}}/images/week10/10-1/CL_tracking.png" style="width: 500px; text-align: center; background-color:#DCDCDC;" /><br>


**What is the fundamental property of Contrastive Learning that prevents trivial solutions?**

<div style="text-align: center;"><img src="{{site.baseurl}}/images/week10/10-1/CL_objective.png" style="width: 400px; background-color:#DCDCDC;" /><br></div>

The objective function of Contrastive Learning is designed to prevent trivial solutions. The aim of the function is to make sure the distance between positive embedding pairs are smaller than the distance between negative embedding pairs. Hence, a trivial solution, where a constant distance is assigned to all embedding pairs, is impossible to attain through a minimization process on the given objective function. However, research has shown that good negative pairs are crucial for Contrastive Learning to work well.




## SimCLR
SimCLR, Simple Framework for Contrastive Learning of Visual Representations, is a contrastive learning method for self supervised learning. Two correlated views of an image are created using augmentation operations like random cropping, resize, color distortions and Gaussian blur. A base encoder is used to obtain representations that are then passed through a projection head. The final representations are used to perform contrastive learning where representations of corresponding pairs are made to “attract” each other and that of non-corresponding pairs to “repel” each other. It utilizes a large batch of images to generate negative samples for use in the contrastive learning task. 

The requirement of having a large batch size can be solved by spreading the batch across multiple GPUs. Images can be passed through multiple GPUs independently. A simple way to create negatives for images from one GPU would be to use the embeddings that are output from other GPUs.

**Advantages of SimCLR:**
1. Simple to implement

**Disadvantages of SimCLR:**
1. Large batch size required
2. Large number of GPUs required

**How do we solve the compute problem in the SimcLR?**

A **Memory bank** can be used to maintain a momentum of activations across all features. Hence, a set of 1000 examples, would mean a memory bank of 1000 features. Every time an image undergoes a forward pass through the network, the memory bank is updated using the new embeddings. These memory bank features can then be used as negatives for contrastive loss. 

#### Advantages of Memory bank:
- It is compute efficient as it requires only one Forward Pass to compute the embeddings to store.

#### Drawbacks of Memory bank:
- Not an online solution, features are only stored once per epoch and hence becomes stale very quickly.
- Needs a large amount of GPU RAM to store the features.
- Hard to scale to millions of images, as storage also needs to scale to millions of features.

## MoCo

MoCo, or, Momentum Contrast, is a contrastive method that utilizes a memory bank to maintain a momentum of activations. It works by using 2 encoders. The $f_{\theta}$ encoder is the encoder that needs to be learnt. $f_{\theta_{EMA}}$ encoder maintains an exponential moving average of $f_{\theta}$.

<img src="{{site.baseurl}}/images/week10/10-1/moco.png" style="width: 600px; background-color:#DCDCDC;" /><br>

During Forward Pass, each sample is forwarded through both encoders. Some of the embeddings from $f_{\theta_{EMA}}$ are kept aside for use as negative embeddings. For the algorithm, original embeddings comes from $f_{\theta}$, positive embeddings from $f_{\theta_{EMA}}$ and negative embeddings come from the set of stored embeddings.

#### Advantages of MoCo:
- Can easily be scaled in terms of memory usage as the full dataset need not be stored.
- Is an online solution, as the moving average is continuously updated.
 
#### Drawbacks of MoCo:
- Two Forward Passes are required, one through $f_{\theta}$ and another through $f_{\theta_{EMA}}$.
- Extra memory required for parameters/stored features.


## Clustering methods
**How CL and clustering are related to one another?**

In Constrastive Learning, each sample has corresponding positives and negatives, and the aim of the learning task is to try and bring together positive embeddings, while pushing apart negative embeddings. Essentially we are creating groups within the feature space.

<img src="{{site.baseurl}}/images/week10/10-1/contrastive-learning.png" style="width: 350px; background-color:#DCDCDC;" /><br>

Clustering, on the other hand, is a more direct way to achieve grouping, as it naturally creates groups in the feature space.

<img src="{{site.baseurl}}/images/week10/10-1/clustering.png" style="width: 250px; background-color:#DCDCDC;" /><br>

## SwAV
The SwAV (Swapping Assignments between Views) algorithm is an online clustering method. The idea here is to maximize the similarity of a given image $I$ and augmentation of that image $augment(I)$ and, in doing so, ensure they belong to the same cluster.

<div class="MathJax_Display" style="text-align: center;">
$$
\begin{aligned}
    f_{\theta}(I) = f_{\theta}(augment(I))
\end{aligned}
$$
</div>

In the images below, samples and their augmentations are represented by different shades of the same color. The different prototypes (or cluster-centers) of the clustering process are present across the x-axis. The similarity of each sample's embedding with each of the prototypes is computed. The sample is then assigned to the prototype with the highest similarity. The aim of the network is to assign an image and its augmentation to the same prototype.

**Can it lead to any trivial solution?**

A trivial solution is possible where every sample is assigned to the same group/cluster/prototype. This would cause a collapsed representation for any given input.


**How to avoid trivial solution?**
1. **Equipartition constraint**

    <img src="{{site.baseurl}}/images/week10/10-1/equipartition.png" style="width: 350px; background-color:#DCDCDC;" /><br>

    The clustering process can be done in a controlled way using an equipartition constraint.  Here, given n samples and k partitions, each cluster is allowed to have a maximum of n/k samples. Embeddings are equally partitioned and hence this prevents the single cluster trivial solution.
    
    To implement this, instead of using a technique such as k-means for assigning clusters to samples, optimal-transport based clustering methods such as the Sinkhorn-Knopp algorithm  can be used that inherently guarantees the equipartition.


2. **Soft assignment**

    <img src="{{site.baseurl}}/images/week10/10-1/soft-assignment.png" style="width: 450px; background-color:#DCDCDC;" /><br>

    In soft assignment (as opposed to hard assignment), instead of a sample belonging to one prototype, we assume that the sample belongs to all prototypes with some proportion of each, such that they all sum up to 1.0. The values of the soft assignment can be treated as a code. This code indicates how each embedding is encoded in the 'prototype' space.

**Consider the case where we have a high class imbalance. If we use the equipartition method, then won't that give inacurate results?**

Using soft codes rather than hard codes solves this problem. Essentially soft assignment creates a lot more classes (logically) than hard assignment. Soft assignment gives a richer representation and hence is less sensitive to $k$ (number of classes) and hence $k/n$. However, if hard assignment is used, the class imbalance and the fixed value of $n/k$ will create inaccuracies.

#### Training SwAV:

<img src="{{site.baseurl}}/images/week10/10-1/swav.png" style="width: 600px; background-color:#DCDCDC;" /><br>

2 crops from an image are taken and embeddings are computed using the network. The Sinkhorn Knopp algorithm is then used to compute 2 corresponding codes. We then try to predict the $2^{nd}$ code from the $1^{st}$ embedding and vice versa. The idea is that if it's possible to predict one's code using the other's embedding, then the network has essentially become invariant to data augmentations. Hence both crops will belong to the same cluster. 

The objective is to minimize the KL divergence between the two codes. Gradients are computed and also backpropagated onto the prototypes. Initially, the prototypes start off being random. The prototypes are then updated in an online manner, throughout the training process.

### Advantages of SwAV:
1. No explicit set of negatives are needed, since there is no contrastive learning.
2. Trivial solutions are avoided using optimal transport based methods.
3. Faster convergence than contrastive learning. 
    - This is because the code space imposes more constraints and the embeddings are not compared directly. This is in contrast to Contrastive Learning, where computations happen in embedding space and hence it has a slower convergence.
4. Faster and has smaller compute requirements than MoCo and SimCLR.
5. Easy to train on a smaller number (4-8) of GPUs.


## Pretraining on ImageNet without labels:
Even though ImageNet without labels can be used for self supervised learning, there is an inherent bias in the dataset due to the curation/hand-selection process involved. ImageNet has the following properties due to curation:
1. Images belong to 1000 specific classes
2. Images contain a prominent object
3. Images have very limited clutter and very few background concepts

## Pretraining on Non-ImageNet data

Pretraining on Non-ImageNet data hurts performance.

<img src="{{site.baseurl}}/images/week10/10-1/non-imagenet.png" style="width: 600px; background-color:#DCDCDC;" /><br>

Consider the above image, if we obtain two separate crops from the image, one crop might have a refrigerator and the other crop could be that of a table or a chair. Hence the learning algorithm would try to create similar embeddings for both refrigerator and table/chair crops, because they belong to the same image. This is not what we expect from the training process.

Real world data has very different distributions. They may sometimes even be cartoon images or memes, and there is no guarantee that an image will contain a single (or sometimes, any) prominent object.