---
lang-ref: ch.10-3
lecturer: Alfredo Canziani
title: Transformer  Encoder-predictor-decoder architecture
authors: Rahul Ahuja, jingshuai jiang 
date: 15th April 2021
---


## The Transformer

Before elaborating the encoder-predictor-decoder architecture, we are going to review two models we've seen before.


### Conditional EBM latent variable architecture


We should be familiar with the terminology of these modules from the previous lectures. 
In the conditional EBM latent variable architecture, we have $\vec{x}$ the conditional variable which goes into a predictor. We have $\vec{y}$ which is the target value. The decoder modules will produce $\tilde{\vec{y}}$ when fed with a latent variable $\vec{z}$ and the output of the predictor. $E$ is the energy function which minimizes the energy between $\tilde{\vec{y}}$ and $\vec{y}$. 


<center>
<img src="{{site.baseurl}}/images/week10/10-3/ebm.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 1:</b> Conditional EBM Model.
</center>

### Autoencoder architecture

In Autoencoder architecture , we observed there is no conditional input but only a target variable. The entire architecture is trying to learn the structure in these target variables. The target value $\vec{y}$ is fed through an encoder module which transforms into a hidden representation space, forcing only the most important information through. And the decoder will make these variables come back to the original target space with a $\tilde{\vec{y}}$. And the cost function will try to minimize the distance between $\tilde{\vec{y}}$ and $\vec{y}$. 



<center>
<img src="{{site.baseurl}}/images/week10/10-3/autoencoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 2:</b> Autoencoder Architecture.
</center>



### Encoder-predictor-decoder architecture

<center>
<img src="{{site.baseurl}}/images/week10/10-3/transformer.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 3:</b> Transformer Architecture.
</center>


In a transformer, $\vec{y}$ (target sentence) is a discrete time signal. It has discrete representation in a time index. The $\vec{y}$ is fed into a unit delay module succeeded by an encoder. The unit delay here transforms $\vec{y}[j] \rightarrow \vec{y}[j-1]$. The only difference with the autoencoder here is this delayed variable. So we can use this structure in the language model to produce the future when given the past.



<center>
<img src="{{site.baseurl}}/images/week10/10-3/unit_delay.png" style="zoom: 50%; background-color:#DCDCDC;" /><br>
<b>Figure 4:</b> Unit Delay Module.
</center>

The observed signal, $\vec{x}$ (source sentence) , is also fed through an encoder. The output of both encoder and delayed encoder are fed into the predictor, which gives a hidden representation $\vec{h}$. This is very similar to denoising autoencoder as the delay module acts as noise in this case. And $\vec{x}$ here makes this entire architecture a conditional delayed denoising autoencoder.

### Encoder and predictor module
You can see the detailed explaination of these modules from last year's slides [here](https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/).


## Decoder module

Contrary to what authors of the Transformer paper define, the decoder module consists of `1D-convolution` and `Add, Norm` blocks. The output of the predictor module is fed to the decoder module and the output of the decoder module is the predicted sentence. We can train this by providing the delayed target sequence. 


<center>
<img src="{{site.baseurl}}/images/week10/10-3/decoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 5:</b> The correct notation of decoder.
</center>