## Distillation
Distillation methods are similarity maximization based methods. Like other SSL methods distillation tries to prevent trivial solutions. It does so by asymmetry in two different ways.
* Asymmetric *learning rule* between student teacher
* Asymmetric *architecture* between student teacher

$$ f_{\theta}^{student}(I) = f_{\theta}^{teacher}(augment(I))$$

### BYOL
BYOL is a distillation technique whose architecure is shown below.
<center>
<img src="{{site.baseurl}}/images/week10/10-3/byol.png" style="background-color:#DCDCDC;" /><br>
</center>

There is an asymmetry in architecture between student teacher as student has an additional prediction head. The gradient backpropagation only happens through Student encoder clearly creating an asymmetry in learning rate. In BYOL there is an additional source of asymmetry which is in weights of student encoder and teacher encoder. Teacher encoder is created as moving average of student encoder. These asymmetries will prevent the model from trivial solutions.

### SimSiam
Recent studies showed that all the three sources of asymmetry discussed in BYOL are not needed to prevent the trivial solutions. In *SimSiam* architecture the student and teacher share the same set of weights and there are two sources of asymmetry.
* In architecture of student encoder with an additional predictor head.
* In learning rate, when backpropagating the gradients are passed only through student encoder but not the teacher encoder. After each epoch, the weights of student encoder are copied to the teacher encoder.

<center>
<img src="{{site.baseurl}}/images/week10/10-3/simsiam.png" style="background-color:#DCDCDC;" /><br>
</center>