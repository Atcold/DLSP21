## SEER: Learning from uncharted Images
Compared to Imagenet dataset, real world images may have different distributions (cartoons, memes) and may or may not have a prominent object. In order to verify if the models work well on images outside of Imagenet dataset we decided to test *Swav* method on large scale data. SEER is *Swav* method tested on billions of unfiltered images.

Following graph compares the fine tune performance of the four models when transfered to Imagenet. Using SEER method, a model can be trained with more than a billion parameters which are going to transfer really well to Imagenet.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/seer_1.png" style="background-color:#DCDCDC;" /><br>
</center>

As shown in the following table, the performance of SEER is comparable to the networks trained on curated data with weak supervision.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/seer_2.png" style="background-color:#DCDCDC;" /><br>
</center>

## AVID + CMA
Audio Visual Instance Discrimination with Cross Modal Agreement is a method that combines *contrastive learning* and *clustering* techniques.

For contrastive leaning on an Audio-Video dataset, when the (audio-video) inputs are passed to the two encoders ($f_a, f_v$) we will get two embeddings (audio and video). The embeddings from the same sample should be close in feature space compared to embeddings from different samples.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/avid.png" style="background-color:#DCDCDC;" /><br>
</center>

To introduce the *clustering*, the notion of the positives and negatives is expanded as shown in the following image. Computing the similarities in the video and audio embeddings from a reference point to all the other samples results in *Positive Set* and *Negative Set*. A sample falls into positive set when both its audio and video embeddings are similar to the reference embeddings.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/cma.png" style="background-color:#DCDCDC;" /><br>
</center>