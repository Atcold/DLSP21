---
lang-ref: ch.07-3
title: Introduction to autoencoders
lecturer: Alfredo Canziani
authors: Vidit Bhargava, Monika Dagar
date: 18 March 2021
---
$\newcommand{\vert}{\boldsymbol}$
$\newcommand{\matr}{\boldsymbol}$
## Applications of Autoencoder
### DALL-E: Creating Images from Text

[![hackmd-github-sync-badge](https://hackmd.io/usp9PiUGRdGQzD93wE8Bsw/badge)](https://hackmd.io/usp9PiUGRdGQzD93wE8Bsw)

A neural network that creates images from text captions. 

<center>
<img src="https://i.imgur.com/FosonFM.jpg" style="background-color:#DCDCDC;" /><br>
<b>Figure 1:</b> DALL-E: Input-Output
</center>

Go to the [website](https://openai.com/blog/dall-e/) and play with the captions! 

## Autoencoder
Let's start with some definitions:

### Definitions
#### Input
$\vert{x}$: is observed during both training and testing 

$\vert{y}$: is observed during training but not testing

$\vert{z}$: is not observed (neither during training nor during testing).

#### Output
$\vert{h}$: is computed from the input (hidden/internal)

$\vert{\tilde{y}}$: is computed from the hidden (predicted $\vert{y}$, ~ means *circa*)

Confused?
Refer to the below figure to understand the use of different variables in different machine learning techniques.
<center>
<img src="https://i.imgur.com/Y4CFlZc.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 2:</b> Variable definitions in different machine learning techniques.
</center>

### Introduction

These kind of networks are used to learn the internal structure of an input and encode it in a hidden internal representation ($\vert{h}$) which expresses the input.

We already learned how to train energy-based models, let's look at the below network:
<center>
<img src="https://i.imgur.com/NAUmcJd.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 3:</b> Autoencoder Architecture.
</center>

Here instead of computing the minimization of the energy $E$ with respect to $\vert{z}$, we use an encoder that approximates the minimization and provides a hidden representation $\vert{h}$ for a given $\vert{y}$.
$$
\vert{h} = \text{Encoder}(\vert{y})
$$

Then the hidden representation is converted into $\vert{\tilde{y}}$ (Here we don't have a predictor, we have an encoder).
$$
\vert{\tilde{y}}= \text{Decoder}(\vert{h})
$$

Basically, $\vert{h}$ is a squashing function $f$ of the rotation of our input $\vert{y}$. ($\vert{y}$ is the observation) and $\vert{\tilde{y}}$ is a squashing function $g$ of the rotation of our hidden representation $\vert{h}$.
$$
\vert{h} = f(\matr{W_h}\vert{y} + \vert{b_h}) \\
\vert{\tilde{y}} = g(\matr{W_y}\vert{h} + \vert{b_y})
$$

Note that, here $\vert{y}$ and $\vert{\tilde{y}}$ both belong to the same input space, and $\vert{h}$ belong to $\mathbb{R}^d$ which is the internal representation. $\matr{W_h}$ and $\matr{W_y}$ are matrices for rotation.

$$
\vert{y}, \vert{\tilde{y}} \in \mathbb{R}^n \\
\vert{h} \in \mathbb{R}^d \\
\matr{W_h} \in \mathbb{R}^{d \times n} \\
\matr{W_y} \in \mathbb{R}^{n \times d}
$$

This is called Autoencoder. Enocder is performeing amortizing and we dont have to minimize the enerzy  $E$ but $F$:

$$
F(\vert{y}) = C(\vert{y},\vert{\tilde{y}}) + R(\vert{h})
$$

### Reconstruction Costs
Below are the two examples of reconstruction energies:
#### Real Valued Input:
$$
C(\vert{y},\vert{\tilde{y}}) = \Vert \vert{y}-\vert{\tilde{y}} \Vert^2 =  \Vert \vert{y}-\text{Dec}[\text{Enc}(\vert{y})] \Vert^2
$$
This is the square ecludian distance between $\vert{y}$ and $\vert{\tilde{y}}$.

#### Binary Input:
In the case of binary input, we can simply use binary cross-entropy

$$
C(\vert{y},\vert{\tilde{y}}) = - \sum_{i=1}^n[{\vert{y_i}\log(\vert{\tilde{y_i}}) + (1-\vert{y_i})\log(1-\vert{\tilde{y_i}})]}
$$

### Loss Functionals
Average across all training samples of per sample loss function

$$
\mathcal{L}(F(\cdot),\matr{Y}) = \frac{1}{m}\sum_{j=1}^m{\ell(F(\cdot),\vert{y}^{(j)})} \in \mathbb{R}
$$
We take the energy loss and try to push the energy down on $\vert{\tilde{y}}$
$$
\ell_{\text{energy}}(F(\cdot),\vert{y}) = F(\vert{y})
$$


### Use-cases
The size of the hidden representation $\vert{h}$ obtained using these networks can be both smaller and larger than the input size. 

If we choose a smaller $\vert{h}$, the network can be used for non-linear dimensionality reduction.

In some situations it can be useful to have a larger than input $\vert{h}$, however, in this scenario, a plain autoencoder would collapse. In other words, since we are trying to reconstruct the input, the model is prone to copying all the input features into the hidden layer and passing it as the output thus essentially behaving as an identity function. This needs to be avoided as this would imply that our model fails to learn anything.

To prevent the model from collapsing, we have to employ techniques that constrain the amount of region which can take zero or low energy values. These techniques can be some sort of regularization such as sparsity constraints, adding additional noise, or sampling.

### Types of Auto-Encoders

#### Denoising autoencoder - 

We add some augmentation/corruption like Gaussian noise to an input sampled from the training manifold $\vert{\hat{y}}$ before feeding it into the model and expect the reconstructed input $\vert{\tilde{y}}$ to be similar to the original input $\vert{y}$.
<center>
<img src="{{site.baseurl}}/images/week07/07-3/DenoisingAutoEncoder.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 4:</b> Denoising Autoencoder Network architecture.
</center>
<!-- ![](https://i.imgur.com/WVcDLns.png) -->
An important note: The noise added to the original input should be similar to what we expect in reality, so the model can easily recover from it.

<center>
<img src="{{site.baseurl}}/images/week07/07-3/DAEOutput.jpeg" style="background-color:#DCDCDC;" /><br>
<b>Figure 5:</b> Network output.
</center>
<!-- ![](https://i.imgur.com/j1CQe3T.jpg) -->
In the image above, the light colour points on the spiral represent the original data manifold. As we add noise, we go farther from the original points. These noise-added points are fed into the auto-encoder to generate this graph. 
The direction of each arrow points to the original datapoint the model pushes the noise-added point towards; whereas the size of the arrow shows by how much. 
We also see a dark purple spiral region which exists because the points in this region are equi-distant from two points on the data manifold. 

