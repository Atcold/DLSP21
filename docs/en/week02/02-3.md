---
lang-ref: ch.2-3
title: Affine transformation and nonlinearity
lecturer: Alfredo Canziani
authors: Jason Soong
date: 17 May 2021
---

## Summary
Summary:
Neural Networks are composed of linear and non-linear blocks.  Affine Transformations are linear transformations which perform the operations of translation/shifting, scaling, rotation, reflection, and sheering.  When you see a matrix, you are performing a linear transformation.  With non-linear functions we can now perform squashing/twisting.  No combination of linear transformations can replicate a non-linear function.

## Practicum
Yeah, contribution and things you can reach out on the Campuswire we handle those administrative questions there, right? All right. Okay. That's very good question. Right. So we start with actually a question from home, which is in the lecture Yann said that in very high dimensional space, there are less local minimum or less likely to have a local minimum. Can you elaborate the intuition behind this, right? Then that's where we starting today, last time I show you. Okay, I will eventually answer this question with in today's session but I need to start with a detour because otherwise not everyone is able to follow since I haven't yet explain exactly what I show you last time. Actually we finished class last time. Okay, hold on many question. Let me finish the first one. 

We finish class last time by, I show you an animation, which I didn't explain some, there is no. How to say, a requirement for you to understand what it was. But I ask a question at the end no? There were five arrows, five arrows in a 2d space. Does anyone have any guesses about what those five arrows in a two dimensional space, were?  You can type on the chat, I can see whether there is any clue.  A basis vector of what? So where can you find those five vectors in this possibly neural network I that I've been used, right? We figure this in a moment, right? Okay, so PCA okay, actually yeah, you're getting kind of close. So I got to do a PCA. Like automatically, I just asked the network to reduce the dimensionality using a linear transformation.  So, what are these linear Transformations, right? So, the only thing there are two things yesterday Yann talk about, right? And so, let me start by sharing. The weight Matrix of the output matrix, yeah. The weights of the output Matrix that is the actually the solution of the question. You want? Mm Bravo. Okay. I'll give an objective but yeah, that's correct. So the answer of my question was those were the weights of the last metrics, right but again, you were not supposed to know the answer. I was just teasing you.

So let's start by sharing my screen share screen and I will share this screen and possibly, you can see something. Yeah. Okay, so this was one slide from Yann, right? I don't like his slides, but okay, we are using this anyway. So this is one of the two slides that are, you know, the most important thing. So, Yann covers, everything horizontally. I just pick one thing and I go down vertically. 

So, the whole thing it says here, is that neural Nets are a bunch of items which are this first item over here, which is called linear block. And then there is another item over here, which is this nonlinear block, right? So and what I call them, so usually. So this is supposedly, I think X is a scalar. So w0 is also a scalar. Usually we end up using a vector in the input and therefore, w0 is actually w Capital W Matrix, right? So whenever you have a w Matrix multiplied by X is going to be linear block and then you have this S1. And S1 goes through these H, which is, I guess, this non-linear, right? So, he calls linear nonlinear since I am weird, I called them differently. And so, first of all, what are the, where is the chat now? Because of course, I share my screen. And you disappear, chat? Okay, one second, I should just share the screen to begin with so I can set up my thing. 

Okay, first of all what is the linear transformation, right? I don't want the technical definition want the intuition, right? So let's say we are in a 2d space. So X has a two dimensional is has two dimensions and then I apply a matrix, which is a 2 by 2 Matrix. So if I apply a 2 by 2 Matrix, To a vector of two Dimensions. What kind of operations do you perform to that point? Right? So a 2d Vector can be a vector, like an arrow in a 2d space. That is pinpointed in the origin. Or you can also consider a point in this 2D space, right? So this can be considered a vector, you know, a point in a 2d space is going to be two components represent this point or this point, or this point, or the other one, right? So if I apply... depends on the Matrix, that's a correct answer. So what are possible outcomes. Let's say you try different matrices what do you expect this point? So you have a cloud of points. Let's say you have a house right? You have 1, 2, 3, you have like a square with a triangle on top. If I apply a matrix, you can get sheer for sure. So you get like a stretch in different directions, you have a rotation for sure. So you can have a rotation around which axis like around, where? Duc. I think it's the name. I don't know if I pronounced correctly. Rotation, how?  If this is the origin and this is my square with a triangle on top, where is this stuff rotating? Yeah, but with respect to what like is rotating where? So this is the origin. And this is my, my item here in the first quadrant. So, if I do a rotation, is this one rotating here, or.  The thing is going to be rotating with respect to the origin, right? So if it's here, we're gonna go over there, right? So we have rotation, we have sheer, we have more stuff. What do you have? Let's say the Matrix is diagonal. You have scaling? Yeah. One more actually. Yeah. One more. How about that? The determinant is negative. What happens? Reflection. Yeah, so if the if the determinant is negative, you get the reflection and then since we actually use affine transformations. So what does affine transformation have in addition with respect to this linear transformation, into the space? So more guy here, shifting, right? Okay. 

So with all these five things, this what usually I think about when, you know, I apply a matrix to this point. Why am I talking about point because, you know, an image or a piece of audio or anything can be just considered very, very, very long Vector with many items in this Vector is just a point in this huge High dimensional space. And so let's say I have many images of a cat, many images of a dog, right? I put them all in big vectors, then I plot them. So let's say this is an image of a cat. This is an image of a cat. This is an image of a cat. I have now image of a dog where, where is going to be ending up? What do you think is going to be here? Here or here. 

You don't see the images. Now, you should see myself doing weird things. Can you see my camera or is it off? No, it should see me, right? Okay. Call this a is in a different region of the space. Any other guess? I do not see you. Well, can people see me? I see. Okay. All right, you should Okay, someone cannot see me. Okay, I understand you see me. Someone did, it doesn't see me. So, I am just moving my hands and maybe you should turn off the screen share. No, no. You should see my screen both. I don't know. Okay, you should be able to see both things. Oh, you had to click on, hide other participants, right? But then, I don't know. Spotlight. Okay. Can you see me now? Hi, you figure son cool. 

All right, so we had images of cats, cats, cats, then I had this image of dogs. Where should I place it here? Here or further away one? Two, three, one, two, three. Then me, So these was one. This is two. This is three. Yeah I know I mean just Jeffrey in just just guess it's okay. All right. So unfortunately everything is going to be just stuck here. Everything is all together mash in one specific place because what matters eventually is that the statistics of these type of data it's very similar and therefore the location in this huge dimensional space will eventually have everything clash in the basically same space. So, That's why it was very hard to tune to you know to make likes a classification because everything is just crammed in a very little space. So what you want to do usually is take this one, you want to zoom in but the origin is over here. So how can you zoom this first if you want to zoom this location, And this is the origin. Someone finish the sentence. You have to shift to the origin, right? So how can I shift to the origin? What operation did I do? What is where is this location? What is this location, right? So, I have many points. How can how can I compute? The, I had to compute the mean, right? So I can figure out where the, the video, what's called the center of mass is of these things. Then I subtract now. You get zero means so everything stays in the origin. So everything is here. So I can now Zoom, right and so actually at the beginning to make things easy. So we said Everything was cramming in a very tiny little space. So how does the network know how close things are? Well let's say I'm using different data, this data comes from a different camera. It comes from a different acquisition system, so points are still very far from the origin but might be more scattered less scatter. You don't exactly know what is the size of this cloud of points, right? So the second point would be such that the network doesn't shouldn't have to care about the size of this cloud is going to be...Normalization, which means. Speaking English. Speaking English scale. What kind of scale divided by the standard deviation? Yes so the standard deviation tells you how it is the spread of this point divided by the spread of the like you divide by this kind of spread such that things are within this kind of, you know, bubble thingy sweet. Now we had this bubble, which is now centering the origin and has kind of radius of, it's not radius of 1 because that's the standard deviation. Sorry radius is Ha ha. Okay. Okay. Keep that. Keep that in mind. Robert, please. I asked you in a second. You answer this question for me in a minute. So I get this Cloud here, right? All right. Cool. 

So I get this Cloud here, right? All right. Cool. And then, all the things I do is going to be apply a linear transformational, which we said are doing scaling, rotation, reflection, and sheering. And then we also have translation since we apply the affine transformation. Okay, okay. And so, usually since those are too many things to say every time, five things, I just pick rotation okay? Whenever I apply a linear transformation, every time I apply a matrix, I just from now on until the end of class we'll talk about rotation which is mostly what happens in high dimensional space. So whenever you apply that Matrix to a vector in blah space, you basically rotate stuff, right? All the other things are less dominant especially because also, you can extract the scaling Factor, you know, with a scalar value. And then if it's negative, you can also extract that and so you can split the effects, right? Of the linear transformation into its own components. And using the Matrix, what it does? I like to think about rotating, maybe it doesn't do just that, but that's my view. Okay. So there is this rotation, I call it and then there is the other thing. So I rotate, and then be happening many other things like zooming and things and then I put inside this nonlinear function which I call squashing, right? And so usually at the end of my course people the only thing when they keep repeating repeating to me know because I repeat this every time what a neural network does are basically two things. First one is rotating data and the secondary squashing rotating, squashing rotating and squashing, right? So that's my perspective View of how this stuff works. All right, enough talking about me and like me my ideas and I show you a few of these things, right? And then, we put together these two things and see how they play now. But now, I'm going to be showing you these two parts, the linear part the nonlinear, so the rotation and the squashing separately, right? Oh, and someone is vacuuming. It's so nice. That's the the teaching from home you know okay. Squashing equal area 0 right now. No squashing. Sorry, no squashing for me means changing things in a nonlinear way, okay? So if you have like a spread of point, maybe I just curl curve, the I apply a nonlinear transformation, so squashing in my jargon means, apply nonlinear transformation. Some kind of weird pushing twisting twisting, yes yes squashing twisting. Yeah twisting that's the better word but I've been using squashing. So yeah, twisting would be better. 

Can you elaborate rotation and squashing a little bit? If I say rotation you just have to think affine transformation. If I say squashing, you have to think nonlinear function. That's it. And the other two words are what I think when I see a matrix or when I see a nonlinear function, okay that's the only point I'm making here. So this slides which is, you know, I will bit more formal which is a linear block no linear blocks in my jargon will be rotating and squashing. Okay?