---
lang-ref: ch.11-2
lecturer: Awni Hannun
title: Speech Recognition and Graph Transformer Network (Part 2)
authors: Gyanesh Gupta, Abed Qaddoumi
date: 14th April 2021
---

## Inference Time

The inference of a transcription from a given audio signal can be formulated using 2 distributions:

* The acoustic model (audio to transcription), represented as $P(Y \mid X)$
* The language model, $P(Y)$

The final inference is obtained by taking sum of the log probabilities of the above two, i.e.

$$ Y* = \underset{Y}{\operatorname{argmax}} \log P(Y \mid X) + \log P(Y) $$

We use the additional term to ensure the transcription is consistent with the rules of the language. Else we may get grammatically wrong transcription.


## Beam Search

While returning an output sequence, we can follow the greedy approach, where we take the maximum value of $$P(y_{t} \mid y_{t-1}...y_{1})$$

However, we can end up missing out a good sequence, which may not have the maximum value of $P(y_{t} \mid ...)$, as illustrated by the example below.

<!--![Shortest Path failing in the Greedy Approach](https://i.imgur.com/DX8j0Sv.png) -->

<center>
<img src="{{site.baseurl}}/images/week11/11-2/greedy.png" style="background-color:#DCDCDC;"/><br>
<b>Figure 1:</b> Greedy Approach : Less Optimal Solution<br>
<br>
</center>

To remedy this, we employ beam search. Essentially, we consider maximum $k$ tokens in terms of probability at each step $t$ of the sequence. For each of these n-grams, we proceed further and find out the maximum. 

The illustration below shows how Beam Search can lead to a better sequence.

<center>
<img src="{{site.baseurl}}/images/week11/11-2/bs3.png" style="background-color:#DCDCDC;"/><br>
<b>Figure 2:</b> Beam Search : Stage 1 <br>
<br>
</center>

<center>
<img src="{{site.baseurl}}/images/week11/11-2/bs1.png" style="background-color:#DCDCDC;"/><br>
<b>Figure 3:</b> Beam Search : Stage 2<br>
<br>
</center>

<center>
<img src="{{site.baseurl}}/images/week11/11-2/bs2.png" style="background-color:#DCDCDC;"/><br>
<b>Figure 4:</b> Beam Search : Stage 3<br>
<br>
</center>


## Graph Transformer Networks

We have previously seen Weighted Finite State Automata (WFSA) being used to represent the alignment graphs, as shown before.


Graph Transformer Networks (GTNs) are basically WSFA with automatic differentiation.

Lets look at key differences between Neural Networks (NNs) and GTNs



|  | NN | GTN |
| -------- | -------- | -------- |
| Core Data Structure     | Tensor     | Graph/WFSA     |
|     | Matrix Multiplcation     | Compose     |
| Core Operations  | Reduction operations (Sum, Product)| Shortest Distance (Viterbi,Forward) |
| | Negate, Add, Subtract, ...     | Closure, Union, Concatenate, ...    |







