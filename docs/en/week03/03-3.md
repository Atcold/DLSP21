|                              |
| ---------------------------- |
| lang-ref: ch.3-3             |
| title: Spiral classification |
| lecturer: Alfredo Canziani   |
| authors: Wenhao Li           |
| date: 6th May, 2021          |
| ---                          |

## Spiral classification
<center>
<img src="{{site.baseurl}}/images\week03\03-3\figure1.png" style="background-color:#DCDCDC;" /><br>
</center>


We use the following formula to generate the spiral figure.

We generate 1000 data points. 

For each data point:

$x\_axis = (t*sin[\frac{2\pi}{K}(2t+k-1)]) + Noise $

$y\_axis = (t*cos[\frac{2\pi}{K}(2t+k-1)])+ Noise $

$0<=t<=1; Red: k = 1, Purple: k = 2, Yellow : k = 3$

If we are given a new pair of x and y coordinates, we want our model is able to output its corresponding color.

<center>
<img src="{{site.baseurl}}/images\week03\03-3\figure2.png" style="background-color:#DCDCDC;" /><br>
</center>

This is a linear model. This model is not good if we only perform linear transformations on the model. We can easily observe that these intersections are very hard to tell apart.

<center>
<img src="{{site.baseurl}}/images\week03\03-3\figure3.png" style="background-color:#DCDCDC;" /><br>
</center>

Therefore we want to improve our model. The idea is simple, just add a nonlinear transformation, say a RELU layer, in the middle. Because the linear transformation makes the dividing line rotate, the nonlinear transformation makes the dividing line squashed. By combining linear and nonlinear transformations, we can obtain various types of dividing line.

## Training data

<center>
<img src="{{site.baseurl}}/images\week03\03-3\figure4.png" style="background-color:#DCDCDC;" /><br>
</center>

Suppose the number of data points is $m$ and the number of dimension is $n$. (In last section, $m = 1000 \space and \space n = 2$)

We use a matrix of $m * n$ denotes our input, and a matrix of $m*1$ denotes our output. Here, $c_i$denotes the color of the $i_{th}$ data points. Note $Red: c_i = 1, Purple: c_i = 2, Yellow : c_i = 3$

Then we apply 1-hot encoding on $c$ to generate $Y$

1-hot encoding means we want to transfer each data point from scalar to a row vector. For example, for the $i_{th}$ data point, if we have $c_i = 2$, we want to transfer it to $y_i = [0, 0, 1]$. We repeat this method on every data points, then we can transfer the matrix $c$ of shape $(m*1)$to $Y$

of shape $(m*K)$

We infer Y from X through a linear transformation and a nonlinear transformation, then it is called a layer of neural network. The input of the $2nd$ layer is just the output of the $1st$ layer. By arranging multiple layers one by one, we get the deep network.

## Backprop

<center>
<img src="{{site.baseurl}}/images\week03\03-3\figure5.png" style="background-color:#DCDCDC;" /><br>
</center>

The last layer is usually a fully-connect layer, and we calculate the loss using the output of this layer.

<center>
<img src="{{site.baseurl}}/images\week03\03-3\figure6.png" style="background-color:#DCDCDC;" /><br>
</center>

As soon as we get the loss( we use notation $J(\theta)$ here), we need to calculate $\frac{\partial J(\theta)}{\partial W} \space and \space \frac{\partial J(\theta)}{\partial b}$ by chain rule. Then we want to update the parameters(e.g., $W \space and \space b$) in the network by Gradient Descent.  

$W = W -  \frac{\partial J(\theta)}{\partial W} * lr$

$b = b - \frac{\partial J(\theta)}{\partial b} * lr$

Since PyTorch has implemented autograd, which is an automatic differentiation engine, we can just simply call loss.backward(). Autograd will automatically calculates the gradients for each model parameter(e.g.,$\frac{\partial J(\theta)}{\partial W} \space and \space \frac{\partial J(\theta)}{\partial b}$  ).

